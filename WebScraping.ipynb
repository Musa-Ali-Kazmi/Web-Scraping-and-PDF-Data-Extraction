{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz4Oeer2qbJI8vQSF9zyQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Musa-Ali-Kazmi/Web-Scraping-and-PDF-Data-Extraction/blob/main/WebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py"
      ],
      "metadata": {
        "id": "GUwWQiU4s0xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WSxN9dskqex"
      },
      "outputs": [],
      "source": [
        "from tabula import read_pdf\n",
        "\n",
        "import pymysql\n",
        "from sqlalchemy import create_engine\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Necessary Libraries\n",
        "!pip install tabula-py requests beautifulsoup4 pandas sqlalchemy Flask openpyxl\n",
        "\n",
        "# Step 2: PDF Data Extraction using Scraping API\n",
        "import tabula\n",
        "\n",
        "def extract_data_from_pdf(pdf_path):\n",
        "    # Use the read_pdf function from tabula to extract tables from PDF\n",
        "    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "\n",
        "    # Process and clean the extracted tables if needed\n",
        "    cleaned_data = process_and_clean_data(tables)\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "# Step 3: Web Scraping for Additional Data\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_additional_data(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract relevant information from the webpage\n",
        "    additional_data = extract_additional_data(soup)\n",
        "\n",
        "    return additional_data\n",
        "\n",
        "def extract_additional_data(soup):\n",
        "    # Example: Extracting text from HTML tags\n",
        "    additional_data = soup.find('div', class_='additional-info').get_text()\n",
        "\n",
        "    return additional_data\n",
        "\n",
        "# Step 4: Data Transformation\n",
        "import pandas as pd\n",
        "\n",
        "def process_and_clean_data(tables):\n",
        "    # Example: Combine multiple tables into a single DataFrame\n",
        "    unified_data = pd.concat(tables, ignore_index=True)\n",
        "\n",
        "    # Add additional cleaning or processing steps as needed\n",
        "\n",
        "    return unified_data\n",
        "\n",
        "# Step 5: Database API Interaction\n",
        "\n",
        "def store_data_in_database(dataframe, db_url):\n",
        "    engine = create_engine(db_url)\n",
        "    dataframe.to_sql('data_table', engine, if_exists='replace')\n",
        "\n",
        "# Step 6: API Endpoint for Data Storage\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/store_data', methods=['POST'])\n",
        "def store_data():\n",
        "    data = request.json\n",
        "\n",
        "    # Store the received data in the database\n",
        "    store_data_in_database(data, os.environ.get('DB_URL'))\n",
        "\n",
        "    return jsonify({'message': 'Data stored successfully'}), 200\n",
        "\n",
        "# Step 7: Download Button for Excel File\n",
        "@app.route('/download_excel', methods=['GET'])\n",
        "def download_excel():\n",
        "    # Retrieve data from the database\n",
        "    data = retrieve_data_from_database(os.environ.get('DB_URL'))\n",
        "\n",
        "    # Convert data to Excel file\n",
        "    excel_file = convert_to_excel(data)\n",
        "\n",
        "    # Return the Excel file as a downloadable attachment\n",
        "    return send_file(excel_file, as_attachment=True, attachment_filename='data.xlsx')\n",
        "\n",
        "# Step 8: Error Handling and Logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='app.log', level=logging.INFO)\n",
        "\n",
        "# Step 9: Documentation\n",
        "# README file explaining how to run the script, including dependencies and API endpoints\n",
        "\n",
        "# Step 10: Testing\n",
        "# Test your script thoroughly with different PDF structures, web pages, and database interactions\n",
        "\n",
        "# Step 11: Finalization\n",
        "# Ensure all code is well-documented, package the script and dependencies, and share with stakeholders\n"
      ],
      "metadata": {
        "id": "hyzLTwuzLGw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}